{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch!\n",
      "{'cluster_name': 'docker-cluster',\n",
      " 'cluster_uuid': 'nkRHUCJSQ-O3ADkv3ZPOkw',\n",
      " 'name': '118de91b32d2',\n",
      " 'tagline': 'You Know, for Search',\n",
      " 'version': {'build_date': '2024-08-05T10:05:34.233336849Z',\n",
      "             'build_flavor': 'default',\n",
      "             'build_hash': '1a77947f34deddb41af25e6f0ddb8e830159c179',\n",
      "             'build_snapshot': False,\n",
      "             'build_type': 'docker',\n",
      "             'lucene_version': '9.11.1',\n",
      "             'minimum_index_compatibility_version': '7.0.0',\n",
      "             'minimum_wire_compatibility_version': '7.17.0',\n",
      "             'number': '8.15.0'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch('http://localhost:9200')\n",
    "client_info = es.info()\n",
    "print('Connected to Elasticsearch!')\n",
    "pprint(client_info.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [{'end_offset': 26,\n",
      "             'position': 0,\n",
      "             'start_offset': 0,\n",
      "             'token': \"I'm so happy!\\n\",\n",
      "             'type': 'word'}]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "response = es.indices.analyze(\n",
    "    char_filter=[\n",
    "        \"html_strip\"\n",
    "    ],\n",
    "    text=\"I&apos;m so happy</b>!</p>\",\n",
    ")\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [{'end_offset': 37,\n",
      "             'position': 0,\n",
      "             'start_offset': 0,\n",
      "             'token': 'I saw comet Tsuchinshan Atlas in 2024',\n",
      "             'type': 'word'}]}\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"keyword\",\n",
    "    char_filter=[\n",
    "        {\n",
    "            \"type\": \"mapping\",\n",
    "            \"mappings\": [\n",
    "                \"٠ => 0\",\n",
    "                \"١ => 1\",\n",
    "                \"٢ => 2\",\n",
    "                \"٣ => 3\",\n",
    "                \"٤ => 4\",\n",
    "                \"٥ => 5\",\n",
    "                \"٦ => 6\",\n",
    "                \"٧ => 7\",\n",
    "                \"٨ => 8\",\n",
    "                \"٩ => 9\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'The', Type: <ALPHANUM>\n",
      "Token: '2', Type: <NUM>\n",
      "Token: 'QUICK', Type: <ALPHANUM>\n",
      "Token: 'Brown', Type: <ALPHANUM>\n",
      "Token: 'Foxes', Type: <ALPHANUM>\n",
      "Token: 'jumped', Type: <ALPHANUM>\n",
      "Token: 'over', Type: <ALPHANUM>\n",
      "Token: 'the', Type: <ALPHANUM>\n",
      "Token: 'lazy', Type: <ALPHANUM>\n",
      "Token: 'dog's', Type: <ALPHANUM>\n",
      "Token: 'bone', Type: <ALPHANUM>\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"standard\",\n",
    "    text=\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}', Type: {token['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'the', Type: word\n",
      "Token: 'quick', Type: word\n",
      "Token: 'brown', Type: word\n",
      "Token: 'foxes', Type: word\n",
      "Token: 'jumped', Type: word\n",
      "Token: 'over', Type: word\n",
      "Token: 'the', Type: word\n",
      "Token: 'lazy', Type: word\n",
      "Token: 'dog', Type: word\n",
      "Token: 's', Type: word\n",
      "Token: 'bone', Type: word\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"lowercase\",\n",
    "    text=\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}', Type: {token['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'The', Type: word\n",
      "Token: '2', Type: word\n",
      "Token: 'QUICK', Type: word\n",
      "Token: 'Brown-Foxes', Type: word\n",
      "Token: 'jumped', Type: word\n",
      "Token: 'over', Type: word\n",
      "Token: 'the', Type: word\n",
      "Token: 'lazy', Type: word\n",
      "Token: 'dog's', Type: word\n",
      "Token: 'bone.', Type: word\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"whitespace\",\n",
    "    text=\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}', Type: {token['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'The'\n",
      "Token: '2'\n",
      "Token: 'QUICK'\n",
      "Token: 'Brown'\n",
      "Token: 'Foxes'\n",
      "Token: 'jumped'\n",
      "Token: 'over'\n",
      "Token: 'the'\n",
      "Token: 'lazy'\n",
      "Token: 'dog'\n",
      "Token: 'bone'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"standard\",\n",
    "    filter=[\n",
    "        \"apostrophe\"\n",
    "    ],\n",
    "    text=\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'I'\n",
      "Token: 'saw'\n",
      "Token: 'comet'\n",
      "Token: 'Tsuchinshan'\n",
      "Token: 'Atlas'\n",
      "Token: 'in'\n",
      "Token: '2024'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"standard\",\n",
    "    filter=[\n",
    "        \"decimal_digit\"\n",
    "    ],\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'I'\n",
      "Token: 'was'\n",
      "Token: 'temoc'\n",
      "Token: 'nahsnihcusT'\n",
      "Token: 'saltA'\n",
      "Token: 'ni'\n",
      "Token: '٤٢٠٢'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"standard\",\n",
    "    filter=[\n",
    "        \"reverse\"\n",
    "    ],\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'i'\n",
      "Token: 'saw'\n",
      "Token: 'comet'\n",
      "Token: 'tsuchinshan'\n",
      "Token: 'atlas'\n",
      "Token: 'in'\n",
      "Token: '٢٠٢٤'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    analyzer=\"standard\",\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'i'\n",
      "Token: 'saw'\n",
      "Token: 'comet'\n",
      "Token: 'tsuchinshan'\n",
      "Token: 'atlas'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    analyzer=\"stop\",\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'I saw comet Tsuchinshan Atlas in ٢٠٢٤'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    analyzer=\"keyword\",\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '1',\n",
      " '_index': 'index_time_example',\n",
      " '_primary_term': 1,\n",
      " '_seq_no': 0,\n",
      " '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      " '_version': 1,\n",
      " 'result': 'created'}\n"
     ]
    }
   ],
   "source": [
    "index_name = \"index_time_example\"\n",
    "settings = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"char_filter\": {\n",
    "                \"ampersand_replacement\": {\n",
    "                    \"type\": \"mapping\",\n",
    "                    \"mappings\": [\"& => and\"]\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"custom_index_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"char_filter\": [\"html_strip\", \"ampersand_replacement\"],\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\"lowercase\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"custom_index_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es.indices.create(index=index_name, body=settings)\n",
    "\n",
    "document = {\n",
    "    \"content\": \"Visit my website https://myuniversehub.com/ & like some images!\"}\n",
    "response = es.index(index=index_name, id=1, body=document)\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Visit my website https://myuniversehub.com/ & like some images!'}\n"
     ]
    }
   ],
   "source": [
    "response = es.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "hits = response.body[\"hits\"][\"hits\"]\n",
    "\n",
    "for hit in hits:\n",
    "    print(hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'visit'\n",
      "Token: 'my'\n",
      "Token: 'website'\n",
      "Token: 'https'\n",
      "Token: 'myuniversehub.com'\n",
      "Token: 'and'\n",
      "Token: 'like'\n",
      "Token: 'some'\n",
      "Token: 'images'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"field\": \"content\",\n",
    "        \"text\": \"Visit my website https://myuniversehub.com/ & like some images!\"\n",
    "    }\n",
    ")\n",
    "\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Visit my website https://myuniversehub.com/ & like some images!'}\n"
     ]
    }
   ],
   "source": [
    "response = es.search(index=index_name, body={\n",
    "    \"query\": {\n",
    "        \"match\": { # match is used for full-text search\n",
    "            \"content\": {\n",
    "                \"query\": \"myuniversehub.com\",\n",
    "                \"analyzer\": \"standard\"  # Using a different analyzer than the one used at index time\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "for hit in hits:\n",
    "    print(hit[\"_source\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
